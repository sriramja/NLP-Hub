{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGSlTzxWdVIK"
      },
      "source": [
        "# PySpark - Introduction\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "w-Cz01dFdVIX",
        "outputId": "4c0e53e7-307f-4782-f256-03d8129d9984"
      },
      "outputs": [],
      "source": [
        "import pyspark\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aydvC8-gdVIZ"
      },
      "source": [
        "## Spark\n",
        "![dist](https://github.com/goodboychan/chans_jupyter/blob/main/_notebooks/image/data_distributed.png?raw=1)\n",
        "- Spark\n",
        "    - Compute accross a distributed cluster.\n",
        "    - Data processed in memory\n",
        "    - Well documented high level API\n",
        "![process](https://github.com/goodboychan/chans_jupyter/blob/main/_notebooks/image/spark_process.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckSCmspKdVIZ"
      },
      "source": [
        "## Connecting to Spark\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f07AmHp4dVIa"
      },
      "source": [
        "### Creating a SparkSession\n",
        "Spin up a local Spark cluster using all available cores. The cluster will be accessible via a SparkSession object.\n",
        "\n",
        "The `SparkSession` class has a builder attribute, which is an instance of the `Builder` class. The `Builder` class exposes three important methods that let you:\n",
        "\n",
        "- specify the location of the master node;\n",
        "- name the application (optional); and\n",
        "- retrieve an existing `SparkSession` or, if there is none, create a new one.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gs2MJ6ECdVIa",
        "outputId": "4a28e460-2109-40c8-91e1-cffde5c9f132"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3.0.0\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Create SparkSession object\n",
        "spark = SparkSession.builder.master('local[*]').appName('test').getOrCreate()\n",
        "\n",
        "# What version of Spark?\n",
        "print(spark.version)\n",
        "\n",
        "# Terminate the cluster\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gofFn5eqdVIc"
      },
      "source": [
        "## Loading Data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdIjDUmudVId"
      },
      "source": [
        "### Loading flights data\n",
        "Load airline flight data from a CSV file. To ensure that the exercise runs quickly these data have been trimmed down to only 50,000 records. A larger dataset in the same format [here](https://assets.datacamp.com/production/repositories/3918/datasets/e1c1a03124fb2199743429e9b7927df18da3eacf/flights-larger.csv).\n",
        "\n",
        "Data dictionary:\n",
        "\n",
        "- `mon` — month (integer between 1 and 12)\n",
        "- `dom` — day of month (integer between 1 and 31)\n",
        "- `dow` — day of week (integer; 1 = Monday and 7 = Sunday)\n",
        "- `org` — origin airport (IATA code)\n",
        "- `mile` — distance (miles)\n",
        "- `carrier` — carrier (IATA code)\n",
        "- `depart` — departure time (decimal hour)\n",
        "- `duration` — expected duration (minutes)\n",
        "- `delay` — delay (minutes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OFOMSOgdVIe",
        "outputId": "c303cb84-1647-421f-f3a8-83d0e0b1c7f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The data contain 275000 records.\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+\n",
            "|mon|dom|dow|carrier|flight|org|mile|depart|duration|delay|\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+\n",
            "| 10| 10|  1|     OO|  5836|ORD| 157|  8.18|      51|   27|\n",
            "|  1|  4|  1|     OO|  5866|ORD| 466|  15.5|     102| null|\n",
            "| 11| 22|  1|     OO|  6016|ORD| 738|  7.17|     127|  -19|\n",
            "|  2| 14|  5|     B6|   199|JFK|2248| 21.17|     365|   60|\n",
            "|  5| 25|  3|     WN|  1675|SJC| 386| 12.92|      85|   22|\n",
            "+---+---+---+-------+------+---+----+------+--------+-----+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- mon: integer (nullable = true)\n",
            " |-- dom: integer (nullable = true)\n",
            " |-- dow: integer (nullable = true)\n",
            " |-- carrier: string (nullable = true)\n",
            " |-- flight: integer (nullable = true)\n",
            " |-- org: string (nullable = true)\n",
            " |-- mile: integer (nullable = true)\n",
            " |-- depart: double (nullable = true)\n",
            " |-- duration: integer (nullable = true)\n",
            " |-- delay: integer (nullable = true)\n",
            "\n",
            "None\n",
            "[('mon', 'int'), ('dom', 'int'), ('dow', 'int'), ('carrier', 'string'), ('flight', 'int'), ('org', 'string'), ('mile', 'int'), ('depart', 'double'), ('duration', 'int'), ('delay', 'int')]\n"
          ]
        }
      ],
      "source": [
        "spark = SparkSession.builder.master('local[*]').appName('flights').getOrCreate()\n",
        "\n",
        "# Read data from CSV file\n",
        "flights = spark.read.csv('./dataset/flights-larger.csv', sep=',', header=True, inferSchema=True,\n",
        "                         nullValue='NA')\n",
        "\n",
        "# Get number of records\n",
        "print(\"The data contain %d records.\" % flights.count())\n",
        "\n",
        "# View the first five records\n",
        "flights.show(5)\n",
        "\n",
        "# Check column data types\n",
        "print(flights.printSchema())\n",
        "print(flights.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHEa7P94dVIg"
      },
      "source": [
        "### Loading SMS spam data\n",
        "\n",
        "The file `sms.csv` contains a selection of SMS messages which have been classified as either 'spam' or 'ham'. There are a total of 5574 SMS, of which 747 have been labelled as spam.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jq3aze3idVIi",
        "outputId": "6e9bda29-b2c3-4313-91ea-aa3abcfbb6be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- label: integer (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
        "\n",
        "# Specify column names and types\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType()),\n",
        "    StructField(\"text\", StringType()),\n",
        "    StructField(\"label\", IntegerType())\n",
        "])\n",
        "\n",
        "# Load data from a delimited file\n",
        "sms = spark.read.csv('./dataset/sms.csv', sep=';', header=False, schema=schema)\n",
        "\n",
        "# Print schema of DataFrame\n",
        "sms.printSchema()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2020-08-10-02-Machine-Learning-with-PySpark-introduction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
