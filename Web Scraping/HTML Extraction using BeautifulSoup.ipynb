{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # pandas for dataframe based data processing and CSV file I/O\n",
    "import requests # for http requests\n",
    "from bs4 import BeautifulSoup # for html parsing and scraping\n",
    "import bs4\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "#from tidylib import tidy_document # for tidying incorrect html\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = '''<div class=\"box\">\n",
    "   <div class=\"css-a9jdxq-Container e1nzkzy80\">\n",
    "      <h2 class=\"css-1dd55xt-Title e1hnx4sx0\">Documents</h2>\n",
    "      <input class=\"css-eo3c1x-Input-input ep3169p0\" \n",
    "\tplaceholder=\"Documents\" \n",
    "\taria-label=\"doctor-search\" \n",
    "\tvalue=\"\" \n",
    "\tstyle=\"margin: 0px 0px 8px;\">\n",
    "      \n",
    "   </div>\n",
    "</div>'''\n",
    "js = HTMLtoJSONParser.to_json(content)\n",
    "print (js)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "class HTMLtoJSONParser(html.parser.HTMLParser):\n",
    "    def __init__(self, raise_exception = True) :\n",
    "        html.parser.HTMLParser.__init__(self)\n",
    "        self.doc  = { }\n",
    "        self.path = []\n",
    "        self.cur  = self.doc\n",
    "        self.line = 0\n",
    "        self.raise_exception = raise_exception\n",
    "         \n",
    "    @property\n",
    "    def json(self):\n",
    "        return self.doc\n",
    "         \n",
    "    @staticmethod\n",
    "    def to_json(content, raise_exception = True):\n",
    "        parser = HTMLtoJSONParser(raise_exception = raise_exception)\n",
    "        parser.feed(content)\n",
    "        return parser.json\n",
    "         \n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        self.path.append(tag)\n",
    "        attrs = { k:v for k,v in attrs }\n",
    "        if tag in self.cur :\n",
    "            if isinstance(self.cur[tag],list) :\n",
    "                self.cur[tag].append(  { \"__parent__\": self.cur } )\n",
    "                self.cur = self.cur[tag][-1]\n",
    "            else :\n",
    "                self.cur[tag] = [ self.cur[tag] ]\n",
    "                self.cur[tag].append(  { \"__parent__\": self.cur } )\n",
    "                self.cur = self.cur[tag][-1]\n",
    "        else :\n",
    "            self.cur[tag] = { \"__parent__\": self.cur }\n",
    "            self.cur = self.cur[tag]\n",
    "             \n",
    "        for a,v in attrs.items():\n",
    "            self.cur[\"#\" + a] = v\n",
    "        self.cur[\"\"] = \"\"\n",
    "                 \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag != self.path[-1] and self.raise_exception :\n",
    "            raise Exception(\"html is malformed around line: {0} (it might be because of a tag <br>, <hr>, <img .. > not closed)\".format(self.line))\n",
    "        del self.path[-1]\n",
    "        memo = self.cur\n",
    "        self.cur = self.cur[\"__parent__\"]\n",
    "        self.clean(memo)\n",
    "                 \n",
    "    def handle_data(self, data):\n",
    "        self.line += data.count(\"\\n\")\n",
    "        if \"\" in self.cur :\n",
    "            self.cur[\"\"] += data\n",
    "             \n",
    "    def clean(self, values):\n",
    "        keys = list(values.keys())\n",
    "        for k in keys:\n",
    "            v = values[k]\n",
    "            if isinstance(v, str) :\n",
    "                #print (\"clean\", k,[v])\n",
    "                c = v.strip(\" \\n\\r\\t\")\n",
    "                if c != v : \n",
    "                    if len(c) > 0 : \n",
    "                        values[k] = c\n",
    "                    else : \n",
    "                        del values[k]\n",
    "        del values[\"__parent__\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "f=codecs.open(\"index.html\", 'r', 'utf-8')\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "dic = dict()\n",
    "\n",
    "\n",
    "itt = 0\n",
    "\n",
    "\n",
    "def list_tree_names(node):\n",
    "    global itt\n",
    "    for child in node.contents:\n",
    "        try:\n",
    "            dic.update({child.name + \"/\" + str(itt): child.attrs})\n",
    "            itt += 1\n",
    "            list_tree_names(node=child)\n",
    "        except:\n",
    "            dic.update({\"text\" + \"/\" + str(itt): child})\n",
    "            itt += 1\n",
    "            \n",
    "            \n",
    "\n",
    "soup = BeautifulSoup(f.read(), \"html.parser\")\n",
    "\n",
    "list_tree_names(soup.find(\"body\"))\n",
    "\n",
    "# for item in soup.contents:\n",
    "#     a = item\n",
    "#     dic.update({item.name + str(itt): item.attrs})\n",
    "#     itt += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xmltojson\n",
    "import json\n",
    "import requests\n",
    "# Sample URL to fetch the html page\n",
    "url = \"https://geeksforgeeks.com\"\n",
    "  \n",
    "# Headers to mimic the browser\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 \\\n",
    "    (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'\n",
    "}\n",
    "  \n",
    "# Get the page through get() method\n",
    "html_response = requests.get(url=url, headers = headers)\n",
    "  \n",
    "# Save the page content as sample.html\n",
    "with open(\"sample.html\", \"w\") as html_file:\n",
    "    html_file.write(html_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "f=codecs.open(\"DOM.html\", 'r', 'utf-8')\n",
    "from html.parser import HTMLParser\n",
    "\n",
    "class MyHTMLParser(HTMLParser):\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        print(\"Start tag:\", tag)\n",
    "\n",
    "    def handle_endtag(self, tag):\n",
    "        print(\"End tag :\", tag)\n",
    "\n",
    "    def handle_data(self, data):\n",
    "        print(\"Data  :\", data)  \n",
    "\n",
    "parser = MyHTMLParser()\n",
    "#parser.feed(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('master.html', 'r') as f:\n",
    "\n",
    "    contents = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(contents, 'lxml')\n",
    "\n",
    "    for child in soup.recursiveChildGenerator():\n",
    "\n",
    "        if child.name:\n",
    "\n",
    "            print(child.name)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import csv\n",
    "from bs4 import BeautifulSoup # for html parsing and scraping\n",
    "f=codecs.open(\"master.html\", 'r', 'utf-8')\n",
    "#document= BeautifulSoup(f.read(), \"html5lib\")\n",
    "#print(document.prettify()) \n",
    "soup = BeautifulSoup(f.read(), 'html5lib') \n",
    "tags=[\"a\",\"abbr\",\"acronym\",\"address\",\"area\",\"bdo\",\"big\",\"blockquote\",\"br\",\"button\",\"caption\",\"cite\",\"code\",\"col\",\"colgroup\",\"dd\",\"del\",\"dfn\",\"dl\",\"DOCTYPE\",\"dt\",\"em\",\"fieldset\",\"form\",\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"hr\",\"i\",\"img\",\"input\",\"ins\",\"kbd\",\"label\",\"legend\",\"li\",\"map\",\"meta\",\"noscript\",\"object\",\"ol\",\"optgroup\",\"option\",\"p\",\"param\",\"pre\",\"q\",\"samp\",\"select\",\"small\",\"strong\",\"sub\",\"sup\",\"table\",\"tbody\",\"td\",\"textarea\",\"tfoot\",\"th\",\"thead\",\"title\",\"tr\",\"tt\",\"ul\",\"var\"]\n",
    "with open('htmlcsv.csv', 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow([\"Tag_Name\", \"Text\",\"Attributes\"])\n",
    "            for i in tags:\n",
    "                #print(\"Tag:\"+i)\n",
    "                for j in (soup.findAll(i)):\n",
    "                    print(j)\n",
    "                    #print(\"Text:\"+j.get_text())\n",
    "                    #print(\"Atrributes:\"+str(j.attrs))\n",
    "\n",
    "                    writer.writerow([str(i),str(j.get_text()),str(j.attrs)])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium.webdriver as webdriver\n",
    "import lxml.html as lh\n",
    "import lxml.html.clean as clean\n",
    "\n",
    "browser = webdriver.Chrome() # Get local session of Chrome\n",
    "browser.get(\"http://localhost:3000\") # Load page\n",
    "\n",
    "content=browser.page_source\n",
    "cleaner=clean.Cleaner()\n",
    "content=cleaner.clean_html(content) \n",
    "doc=lh.fromstring(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "BASE_URL = \"https://www.awesome-nlp.com\"\n",
    "\n",
    "\n",
    "\n",
    "def get_category_links(section_url):\n",
    "    # Put the stuff you see when using Inspect Element in a variable called html.\n",
    "    html = urlopen(section_url).read()    \n",
    "    # Parse the stuff.\n",
    "    soup = BeautifulSoup(html, \"lxml\")    \n",
    "    # The next two lines will change depending on what you're looking for. This \n",
    "    # line is looking for <dl class=\"boccat\">.  \n",
    "    boccat = soup.find(\"dl\", \"boccat\")    \n",
    "    # This line organizes what is found in the above line into a list of \n",
    "    # hrefs (i.e. links). \n",
    "    category_links = [BASE_URL + dd.a[\"href\"] for dd in boccat.findAll(\"dd\")]\n",
    "    return category_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
